{
  "id": "1afaaa0d-72d4-449a-b333-e305b5d30960",
  "revision": 0,
  "last_node_id": 292,
  "last_link_id": 437,
  "nodes": [
    {
      "id": 146,
      "type": "HiDreamModelNode",
      "pos": [
        -2131.632568359375,
        -2813.61865234375
      ],
      "size": [
        470,
        154
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "HiDreamModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        "CogFlorence-2.1-Large",
        true,
        128,
        false
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 134,
      "type": "LTXVideoModelNode",
      "pos": [
        -3526.18896484375,
        -2956.0810546875
      ],
      "size": [
        690.213134765625,
        130
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "LTXVideoModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        true,
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        1
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 149,
      "type": "CosmosPredict2ModelNode",
      "pos": [
        -3526.02294921875,
        -2627.000732421875
      ],
      "size": [
        643.4683227539062,
        106
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "CosmosPredict2ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        "BOPB",
        ""
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 144,
      "type": "ChromaModelNode",
      "pos": [
        -2797.96533203125,
        -2964.46533203125
      ],
      "size": [
        664.3506469726562,
        106
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "ChromaModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        true
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 216,
      "type": "EditModelDatasetPathNode",
      "pos": [
        -4973.2060546875,
        -1050.393798828125
      ],
      "size": [
        552.0827026367188,
        82
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "input_path",
          "type": "input_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "EditModelDatasetPathNode",
        "ue_properties": {
          "widget_ue_connectable": {
            "target_path": true,
            "control_path": true
          },
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "",
        ""
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 90,
      "type": "FrameBucketsNode",
      "pos": [
        -4900.22998046875,
        -905.724365234375
      ],
      "size": [
        450,
        58
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "frame_buckets",
          "type": "frame_buckets",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "FrameBucketsNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "[1, 33, 65, 97]"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 91,
      "type": "ArBucketsNode",
      "pos": [
        -4892.70361328125,
        -795.7151489257812
      ],
      "size": [
        440,
        60
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "ar_buckets",
          "type": "ar_buckets",
          "links": [
            190
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "ArBucketsNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "[[512, 512], [448, 576],[1024,1024]]"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 232,
      "type": "Note",
      "pos": [
        -1840,
        140
      ],
      "size": [
        405.556884765625,
        88
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "ä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹ä½ æƒ³çœ‹çš„ä»»ä½•å†å²è¿›ç¨‹\nYou can also view any historical process you want to see\n"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 145,
      "type": "MarkdownNote",
      "pos": [
        -5419.77490234375,
        -1979.2459716796875
      ],
      "size": [
        880,
        640
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "# æ‘˜è¦\n| Model          | LoRA | Full Fine Tune | fp8/quantization |\n|----------------|------|----------------|------------------|\n|SDXL            |âœ…    |âœ…              |âŒ                |\n|Flux            |âœ…    |âœ…              |âœ…                |\n|LTX-Video       |âœ…    |âŒ              |âŒ                |\n|HunyuanVideo    |âœ…    |âŒ              |âœ…                |\n|Cosmos          |âœ…    |âŒ              |âŒ                |\n|Lumina Image 2.0|âœ…    |âœ…              |âŒ                |\n|Wan2.1          |âœ…    |âœ…              |âœ…                |\n|Chroma          |âœ…    |âœ…              |âœ…                |\n|HiDream         |âœ…    |âŒ              |âœ…                |\n|SD3             |âœ…    |âŒ              |âœ…                |\n|Cosmos-Predict2 |âœ…    |âœ…              |âœ…                |\n|OmniGen2        |âœ…    |âŒ              |âŒ                |\n|Flux Kontext    |âœ…    |âœ…              |âœ…                |\n|Wan2.2          |âœ…    |âœ…              |âœ…                |\n|Qwen-Image      |âœ…    |âœ…              |âœ…                |\n|Qwen-Image-Edit |âœ…    |âœ…              |âœ…                |\n|HunyuanImage-2.1|âœ…    |âœ…              |âœ…                |\n\n\n\n## SDXL\n```\n[model]\ntype = 'sdxl'\ncheckpoint_path = '/data2/imagegen_models/sdxl/sd_xl_base_1.0_0.9vae.safetensors'\ndtype = 'bfloat16'\n# ä½ å¯ä»¥é€šè¿‡è®¾ç½®æ­¤é€‰é¡¹æ¥è®­ç»ƒv-predictionæ¨¡å‹ï¼ˆä¾‹å¦‚NoobAI vpredï¼‰ã€‚\n#v_pred = true\n# æ”¯æŒæœ€å°SNRã€‚å«ä¹‰ä¸sd-scriptsç›¸åŒ\n#min_snr_gamma = 5\n# æ”¯æŒå»åä¼°è®¡æŸå¤±ã€‚å«ä¹‰ä¸sd-scriptsç›¸åŒã€‚\n#debiased_estimation_loss = true\n# ä½ å¯ä»¥ä¸ºunetå’Œæ–‡æœ¬ç¼–ç å™¨è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªæœªè®¾ç½®ï¼Œåˆ™å°†åº”ç”¨ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€‚\nunet_lr = 4e-5\ntext_encoder_1_lr = 2e-5\ntext_encoder_2_lr = 2e-5\n```\nä¸å…¶ä»–æ¨¡å‹ä¸åŒï¼Œå¯¹äºSDXLï¼Œæ–‡æœ¬åµŒå…¥ä¸ä¼šè¢«ç¼“å­˜ï¼Œå¹¶ä¸”æ–‡æœ¬ç¼–ç å™¨ä¼šè¢«è®­ç»ƒã€‚\n\nSDXLå¯ä»¥è¿›è¡Œå…¨é‡å¾®è°ƒã€‚åªéœ€åˆ é™¤é…ç½®æ–‡ä»¶ä¸­çš„[adapter]è¡¨å³å¯ã€‚ä½ å°†éœ€è¦48GBæ˜¾å­˜ã€‚2Ã—24GB GPUåœ¨è®¾ç½®pipeline_stages=2æ—¶å¯ä»¥å·¥ä½œã€‚\n\nSDXLçš„LoRAä»¥Kohya sd-scriptsæ ¼å¼ä¿å­˜ã€‚SDXLçš„å…¨é‡å¾®è°ƒæ¨¡å‹ä»¥åŸå§‹SDXLæ£€æŸ¥ç‚¹æ ¼å¼ä¿å­˜ã€‚\n\n## Flux\n```\n[model]\ntype = 'flux'\n# Fluxçš„Huggingface Diffusersç›®å½•è·¯å¾„\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\n# ä½ å¯ä»¥ä»BFLæ ¼å¼çš„æ£€æŸ¥ç‚¹ä¸­è¦†ç›–transformerã€‚\n#transformer_path = '/data2/imagegen_models/flux-dev-single-files/consolidated_s6700-schnell.safetensors'\ndtype = 'bfloat16'\n# Fluxåœ¨è®­ç»ƒLoRAæ—¶æ”¯æŒtransformerä½¿ç”¨fp8ã€‚\ntransformer_dtype = 'float8'\n# ä¾èµ–åˆ†è¾¨ç‡çš„æ—¶é—´æ­¥åç§»ï¼Œæœå‘æ›´å¤šå™ªå£°ã€‚å«ä¹‰ä¸sd-scriptsç›¸åŒã€‚\nflux_shift = true\n# å¯¹äºFLEX.1-alphaï¼Œä½ å¯ä»¥ç»•è¿‡å¼•å¯¼åµŒå…¥ï¼Œè¿™æ˜¯è®­ç»ƒè¯¥æ¨¡å‹çš„æ¨èæ–¹å¼ã€‚\n#bypass_guidance_embedding = true\n```\nå¯¹äºFluxï¼Œä½ å¯ä»¥é€šè¿‡å°†transformer_pathè®¾ç½®ä¸ºåŸå§‹é»‘æ£®æ—å®éªŒå®¤ï¼ˆBFLï¼‰æ ¼å¼çš„æ£€æŸ¥ç‚¹æ¥è¦†ç›–transformeræƒé‡ã€‚ä¾‹å¦‚ï¼Œä¸Šé¢çš„é…ç½®ä»Diffusersæ ¼å¼çš„FLUX.1-devåŠ è½½æ¨¡å‹ï¼Œä½†å¦‚æœå–æ¶ˆæ³¨é‡Štransformer_pathï¼Œåˆ™ä¼šä»Flux Dev De-distillåŠ è½½transformerã€‚\n\nFluxçš„LoRAä»¥Diffusersæ ¼å¼ä¿å­˜ã€‚\n\n## LTX-Video\n```\n[model]\ntype = 'ltx-video'\ndiffusers_path = '/data2/imagegen_models/LTX-Video'\n# å°†æ­¤æŒ‡å‘å…¶ä¸­ä¸€ä¸ªå•ä¸€æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»¥ä»ä¸­åŠ è½½transformerå’ŒVAEã€‚\nsingle_file_path = '/data2/imagegen_models/LTX-Video/ltx-video-2b-v0.9.1.safetensors'\ndtype = 'bfloat16'\n# å¯ä»¥ä»¥fp8åŠ è½½transformerã€‚\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n# ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘å¸§ä½œä¸ºæ¡ä»¶çš„æ¦‚ç‡ï¼ˆå³i2vè®­ç»ƒï¼‰ã€‚\n#first_frame_conditioning_p = 1.0\n```\nä½ å¯ä»¥é€šè¿‡ä½¿ç”¨single_file_pathæ¥è®­ç»ƒæ›´æ–°çš„LTX-Videoç‰ˆæœ¬ã€‚è¯·æ³¨æ„ï¼Œä½ ä»ç„¶éœ€è¦å°†diffusers_pathè®¾ç½®ä¸ºåŸå§‹æ¨¡å‹æ–‡ä»¶å¤¹ï¼ˆå®ƒä»è¿™é‡Œè·å–æ–‡æœ¬ç¼–ç å™¨ï¼‰ã€‚ä»…æ”¯æŒt2iå’Œt2vè®­ç»ƒã€‚\n\nLTX-Videoçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## HunyuanVideo\n```\n[model]\ntype = 'hunyuan-video'\n# å¯ä»¥å®Œå…¨ä»ä¸ºå®˜æ–¹æ¨ç†è„šæœ¬è®¾ç½®çš„ckptè·¯å¾„åŠ è½½ Hunyuan Videoã€‚\n#ckpt_path = '/home/anon/HunyuanVideo/ckpts'\n# æˆ–è€…ä½ å¯ä»¥é€šè¿‡æŒ‡å‘æ‰€æœ‰ComfyUIæ–‡ä»¶æ¥åŠ è½½å®ƒã€‚\ntransformer_path = '/data2/imagegen_models/hunyuan_video_comfyui/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors'\nvae_path = '/data2/imagegen_models/hunyuan_video_comfyui/hunyuan_video_vae_bf16.safetensors'\nllm_path = '/data2/imagegen_models/hunyuan_video_comfyui/llava-llama-3-8b-text-encoder-tokenizer'\nclip_path = '/data2/imagegen_models/hunyuan_video_comfyui/clip-vit-large-patch14'\n# æ‰€æœ‰æ¨¡å‹ä½¿ç”¨çš„åŸºç¡€æ•°æ®ç±»å‹ã€‚\ndtype = 'bfloat16'\n# Hunyuan Videoåœ¨è®­ç»ƒLoRAæ—¶æ”¯æŒtransformerä½¿ç”¨fp8ã€‚\ntransformer_dtype = 'float8'\n# ç”¨äºè®­ç»ƒçš„æ—¶é—´æ­¥é‡‡æ ·æ–¹æ³•ã€‚å¯ä»¥æ˜¯logit_normalæˆ–uniformã€‚\ntimestep_sample_method = 'logit_normal'\n```\nHunyuanVideoçš„LoRAä»¥Diffusersé£æ ¼çš„æ ¼å¼ä¿å­˜ã€‚é”®æ ¹æ®åŸå§‹æ¨¡å‹å‘½åï¼Œå¹¶ä»¥â€œtransformer.â€ä¸ºå‰ç¼€ã€‚æ­¤æ ¼å¼å¯ç›´æ¥åœ¨ComfyUIä¸­ä½¿ç”¨ã€‚\n\n## Cosmos\n```\n[model]\ntype = 'cosmos'\n# å°†è¿™äº›è·¯å¾„æŒ‡å‘ComfyUIæ–‡ä»¶ã€‚\ntransformer_path = '/data2/imagegen_models/cosmos/cosmos-1.0-diffusion-7b-text2world.pt'\nvae_path = '/data2/imagegen_models/cosmos/cosmos_cv8x8x8_1.0.safetensors'\ntext_encoder_path = '/data2/imagegen_models/cosmos/oldt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\n```\nå·²åˆæ­¥æ”¯æŒCosmosï¼ˆtext2worldæ‰©æ•£å˜ä½“ï¼‰ã€‚ä¸HunyuanVideoç›¸æ¯”ï¼ŒCosmosåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿›è¡Œå¾®è°ƒå¹¶ä¸ç†æƒ³ã€‚\n\n1. Cosmosæ”¯æŒå›ºå®šçš„ã€æœ‰é™çš„åˆ†è¾¨ç‡å’Œå¸§é•¿åº¦é›†åˆã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œ7bæ¨¡å‹çš„è®­ç»ƒå®é™…ä¸Šæ¯”HunyuanVideoï¼ˆ12bå‚æ•°ï¼‰æ›´æ…¢ï¼Œå› ä¸ºä½ ä¸èƒ½åƒä½¿ç”¨Hunyuané‚£æ ·é€šè¿‡åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸Šè®­ç»ƒæ¥èŠ‚çœèµ„æºã€‚è€Œä¸”è§†é¢‘è®­ç»ƒå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œé™¤éä½ æœ‰å¤§é‡çš„æ˜¾å­˜ï¼Œå› ä¸ºå¯¹äºè§†é¢‘ï¼Œä½ å¿…é¡»ä½¿ç”¨å®Œæ•´çš„121å¸§é•¿åº¦ã€‚\n2. Cosmosåœ¨ä»çº¯å›¾åƒè®­ç»ƒåˆ°è§†é¢‘çš„æ³›åŒ–æ–¹é¢ä¼¼ä¹è¦å·®å¾—å¤šã€‚\n3. CosmosåŸºç¡€æ¨¡å‹åœ¨å…¶äº†è§£çš„å†…å®¹ç±»å‹æ–¹é¢å—åˆ°æ›´å¤šé™åˆ¶ï¼Œè¿™ä½¿å¾—é’ˆå¯¹å¤§å¤šæ•°æ¦‚å¿µçš„å¾®è°ƒæ›´åŠ å›°éš¾ã€‚\n\næˆ‘å¯èƒ½ä¸ä¼šç»§ç»­ç§¯ææ”¯æŒCosmosã€‚æ‰€æœ‰å¿…è¦çš„éƒ¨åˆ†éƒ½å·²å…·å¤‡ï¼Œå¦‚æœä½ çœŸçš„æƒ³å°è¯•è®­ç»ƒå®ƒï¼Œæ˜¯å¯ä»¥åšåˆ°çš„ã€‚ä½†å¦‚æœå‡ºç°é—®é¢˜ï¼Œä¸è¦æœŸæœ›æˆ‘ä¼šèŠ±æ—¶é—´å»ä¿®å¤ã€‚\n\nCosmosçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## Lumina Image 2.0\n```\n[model]\ntype = 'lumina_2'\n# å°†è¿™äº›è·¯å¾„æŒ‡å‘ComfyUIæ–‡ä»¶ã€‚\ntransformer_path = '/data2/imagegen_models/lumina-2-single-files/lumina_2_model_bf16.safetensors'\nllm_path = '/data2/imagegen_models/lumina-2-single-files/gemma_2_2b_fp16.safetensors'\nvae_path = '/data2/imagegen_models/lumina-2-single-files/flux_vae.safetensors'\ndtype = 'bfloat16'\nlumina_shift = true\n```\nå‚è§[Lumina 2ç¤ºä¾‹æ•°æ®é›†é…ç½®](../examples/recommended_lumina_dataset_config.toml)ï¼Œå…¶ä¸­å±•ç¤ºäº†å¦‚ä½•æ·»åŠ æ ‡é¢˜å‰ç¼€å¹¶åŒ…å«æ¨èçš„åˆ†è¾¨ç‡è®¾ç½®ã€‚\n\né™¤äº†LoRAä¹‹å¤–ï¼ŒLumina 2è¿˜æ”¯æŒå…¨é‡å¾®è°ƒã€‚å®ƒå¯ä»¥åœ¨å•ä¸ª24GB GPUä¸Šä»¥1024x1024åˆ†è¾¨ç‡è¿›è¡Œå¾®è°ƒã€‚å¯¹äºå…¨é‡å¾®è°ƒï¼Œåˆ é™¤æˆ–æ³¨é‡Šæ‰é…ç½®ä¸­çš„[adapter]å—ã€‚å¦‚æœåœ¨24GBæ˜¾å­˜ä¸‹è¿›è¡Œå…¨é‡å¾®è°ƒï¼Œä½ éœ€è¦ä½¿ç”¨æ›¿ä»£ä¼˜åŒ–å™¨æ¥å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼š\n```\n[optimizer]\ntype = 'adamw8bitkahan'\nlr = 5e-6\nbetas = [0.9, 0.99]\nweight_decay = 0.01\neps = 1e-8\ngradient_release = true\n```\n\nè¿™ä½¿ç”¨äº†å¸¦æœ‰Kahanæ±‚å’Œçš„è‡ªå®šä¹‰AdamW8bitä¼˜åŒ–å™¨ï¼ˆbf16è®­ç»ƒæ‰€éœ€ï¼‰ï¼Œå¹¶å¯ç”¨äº†å®éªŒæ€§çš„æ¢¯åº¦é‡Šæ”¾ä»¥èŠ‚çœæ›´å¤šæ˜¾å­˜ã€‚å¦‚æœä½ ä»…åœ¨512åˆ†è¾¨ç‡ä¸‹è®­ç»ƒï¼Œå¯ä»¥ç§»é™¤æ¢¯åº¦é‡Šæ”¾éƒ¨åˆ†ã€‚å¦‚æœä½ æœ‰>24GBçš„GPUï¼Œæˆ–è€…å¤šä¸ªGPUå¹¶ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œï¼Œæˆ–è®¸å¯ä»¥ç›´æ¥ä½¿ç”¨æ™®é€šçš„adamwä¼˜åŒ–å™¨ç±»å‹ã€‚\n\nLumina 2çš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## Wan2.1\n```\n[model]\ntype = 'wan'\nckpt_path = '/data2/imagegen_models/Wan2.1-T2V-1.3B'\ndtype = 'bfloat16'\n# è®­ç»ƒLoRAæ—¶ï¼Œä½ å¯ä»¥ä¸ºtransformerä½¿ç”¨fp8ã€‚\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\n\næ”¯æŒt2vå’Œi2vçš„Wan2.1å˜ä½“ã€‚å°†ckpt_pathè®¾ç½®ä¸ºåŸå§‹æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•ï¼Œä¾‹å¦‚[Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B)ã€‚\n\nï¼ˆå¯é€‰ï¼‰ä½ å¯ä»¥è·³è¿‡ä»åŸå§‹æ£€æŸ¥ç‚¹ä¸‹è½½transformerå’ŒUMT5æ–‡æœ¬ç¼–ç å™¨ï¼Œè€Œæ˜¯ä¼ å…¥ComfyUI safetensorsæ–‡ä»¶çš„è·¯å¾„ã€‚\n\nä¸‹è½½æ£€æŸ¥ç‚¹ä½†è·³è¿‡transformerå’ŒUMT5ï¼š\n```\nhuggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir Wan2.1-T2V-1.3B --exclude \"diffusion_pytorch_model*\" \"models_t5*\"\n```\n\nç„¶åä½¿ç”¨æ­¤é…ç½®ï¼š\n```\n[model]\ntype = 'wan'\nckpt_path = '/data2/imagegen_models/Wan2.1-T2V-1.3B'\ntransformer_path = '/data2/imagegen_models/wan_comfyui/wan2.1_t2v_1.3B_bf16.safetensors'\nllm_path = '/data2/imagegen_models/wan_comfyui/wrapper/umt5-xxl-enc-bf16.safetensors'\ndtype = 'bfloat16'\n# è®­ç»ƒLoRAæ—¶ï¼Œä½ å¯ä»¥ä¸ºtransformerä½¿ç”¨fp8ã€‚\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nä½ ä»ç„¶éœ€è¦ckpt_pathï¼Œåªæ˜¯å®ƒå¯ä»¥ç¼ºå°‘transformeræ–‡ä»¶å’Œ/æˆ–UMT5ã€‚transformer/UMT5å¯ä»¥ä»åŸç”ŸComfyUIé‡æ–°æ‰“åŒ…çš„æ–‡ä»¶ï¼Œæˆ–Kijaiçš„åŒ…è£…å™¨æ‰©å±•çš„æ–‡ä»¶ä¸­åŠ è½½ã€‚æ­¤å¤–ï¼Œä½ å¯ä»¥æ··åˆæ­é…ç»„ä»¶ï¼Œä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒä¸­ä½¿ç”¨æ¥è‡ªComfyUIé‡æ–°æ‰“åŒ…ä»“åº“çš„transformerä¸æ¥è‡ªKijaiçš„åŒ…è£…å™¨ä»“åº“çš„UMT5 safetensorsï¼Œæˆ–å…¶ä»–ç»„åˆã€‚\n\nå¯¹äºi2vè®­ç»ƒï¼Œä½ **å¿…é¡»**åœ¨ä»…åŒ…å«è§†é¢‘çš„æ•°æ®é›†ä¸Šè®­ç»ƒã€‚å¦åˆ™è®­ç»ƒè„šæœ¬ä¼šå‡ºé”™å´©æºƒã€‚æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„ç¬¬ä¸€å¸§ç”¨ä½œå›¾åƒæ¡ä»¶ï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥é¢„æµ‹è§†é¢‘çš„å…¶ä½™éƒ¨åˆ†ã€‚è¯·æ³¨æ„video_clip_modeè®¾ç½®ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå®ƒé»˜è®¤ä¸º'single_beginning'ï¼Œè¿™å¯¹äºi2vè®­ç»ƒæ˜¯åˆç†çš„ï¼Œä½†å¦‚æœä½ åœ¨t2vè®­ç»ƒæœŸé—´å°†å…¶è®¾ç½®ä¸ºå…¶ä»–å€¼ï¼Œå¯¹äºi2vå¯èƒ½ä¸æ˜¯ä½ æƒ³è¦çš„ã€‚åªæœ‰14Bæ¨¡å‹æœ‰i2vå˜ä½“ï¼Œå¹¶ä¸”å®ƒéœ€è¦åœ¨è§†é¢‘ä¸Šè®­ç»ƒï¼Œå› æ­¤æ˜¾å­˜è¦æ±‚å¾ˆé«˜ã€‚å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œè¯·æ ¹æ®éœ€è¦ä½¿ç”¨å—äº¤æ¢ã€‚\n\nWan2.1çš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## Chroma\n```\n[model]\ntype = 'chroma'\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\ntransformer_path = '/data2/imagegen_models/chroma/chroma-unlocked-v10.safetensors'\ndtype = 'bfloat16'\n# è®­ç»ƒLoRAsæ—¶ï¼Œä½ å¯ä»¥é€‰æ‹©ä»¥fp8åŠ è½½transformerã€‚\ntransformer_dtype = 'float8'\nflux_shift = true\n```\nChromaæ˜¯ä¸€ä¸ªåœ¨æ¶æ„ä¸Šç»è¿‡ä¿®æ”¹å¹¶ä»Flux Schnellå¾®è°ƒè€Œæ¥çš„æ¨¡å‹ã€‚è¿™äº›ä¿®æ”¹éå¸¸æ˜¾è‘—ï¼Œä»¥è‡³äºå®ƒæœ‰è‡ªå·±çš„æ¨¡å‹ç±»å‹ã€‚å°†transformer_pathè®¾ç½®ä¸ºChromaå•ä¸€æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶å°†diffusers_pathè®¾ç½®ä¸ºFlux Devæˆ–Schnell Diffusersæ–‡ä»¶å¤¹ï¼ˆéœ€è¦Diffusersæ¨¡å‹æ¥åŠ è½½VAEå’Œæ–‡æœ¬ç¼–ç å™¨ï¼‰ã€‚\n\nChromaçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## HiDream\n```\n[model]\ntype = 'hidream'\ndiffusers_path = '/data/imagegen_models/HiDream-I1-Full'\nllama3_path = '/data2/models/Meta-Llama-3.1-8B-Instruct'\nllama3_4bit = true\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n# å¯ä»¥ä½¿ç”¨nf4é‡åŒ–ä»¥èŠ‚çœæ›´å¤šæ˜¾å­˜ã€‚\n#transformer_dtype = 'nf4'\nmax_llama3_sequence_length = 128\n# å¯ä»¥ä½¿ç”¨ä¾èµ–åˆ†è¾¨ç‡çš„æ—¶é—´æ­¥åç§»ï¼Œå¦‚Fluxã€‚ä¸ç¡®å®šç»“æœæ˜¯å¦æ›´å¥½ã€‚\n#flux_shift = true\n```\n\nä»…æµ‹è¯•äº†å®Œæ•´ç‰ˆã€‚Devå’ŒFastç‰ˆæœ¬å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå› ä¸ºå®ƒä»¬ç»è¿‡è’¸é¦ï¼Œå¹¶ä¸”ä½ æ— æ³•è®¾ç½®å¼•å¯¼å€¼ã€‚\n\n**HiDreamåœ¨ä½äº1024çš„åˆ†è¾¨ç‡ä¸‹è¡¨ç°ä¸ä½³**ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸Fluxç›¸åŒçš„è®­ç»ƒç›®æ ‡å’ŒVAEï¼Œå› æ­¤ä¸¤è€…çš„æŸå¤±å€¼å¯ä»¥ç›´æ¥æ¯”è¾ƒã€‚å½“æˆ‘ä¸Fluxæ¯”è¾ƒæ—¶ï¼Œåœ¨768åˆ†è¾¨ç‡ä¸‹æŸå¤±å€¼æœ‰é€‚åº¦ä¸‹é™ã€‚åœ¨512åˆ†è¾¨ç‡ä¸‹æŸå¤±å€¼æœ‰ä¸¥é‡ä¸‹é™ï¼Œå¹¶ä¸”åœ¨512åˆ†è¾¨ç‡ä¸‹æ¨ç†ä¼šäº§ç”Ÿå®Œå…¨å¤±çœŸçš„å›¾åƒã€‚\n\nå®˜æ–¹æ¨ç†ä»£ç å¯¹æ‰€æœ‰æ–‡æœ¬ç¼–ç å™¨ä½¿ç”¨128çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚ä½ å¯ä»¥é€šè¿‡æ›´æ”¹max_llama3_sequence_lengthæ¥æ›´æ”¹llama3ï¼ˆæ‰¿æ‹…å‡ ä¹æ‰€æœ‰æƒé‡ï¼‰çš„åºåˆ—é•¿åº¦ã€‚å€¼ä¸º256ä¼šå¯¼è‡´æ¨¡å‹åœ¨ä»»ä½•è®­ç»ƒå¼€å§‹ä¹‹å‰çš„ç¨³å®šéªŒè¯æŸå¤±ç•¥æœ‰å¢åŠ ï¼Œå› æ­¤å­˜åœ¨ä¸€äº›è´¨é‡ä¸‹é™ã€‚å¦‚æœä½ çš„è®¸å¤šæ ‡é¢˜é•¿äº128ä¸ªä»¤ç‰Œï¼Œå¯èƒ½å€¼å¾—å¢åŠ æ­¤å€¼ï¼Œä½†è¿™æœªç»æµ‹è¯•ã€‚æˆ‘ä¸ä¼šå°†å…¶å¢åŠ åˆ°256ä»¥ä¸Šã€‚\n\nç”±äºLlama3æ–‡æœ¬åµŒå…¥çš„è®¡ç®—æ–¹å¼ï¼ŒLlama3æ–‡æœ¬ç¼–ç å™¨å¿…é¡»åœ¨è®­ç»ƒæœŸé—´ä¿æŒåŠ è½½çŠ¶æ€å¹¶è®¡ç®—å…¶åµŒå…¥ï¼Œè€Œä¸æ˜¯é¢„å…ˆç¼“å­˜ã€‚å¦åˆ™ï¼Œç¼“å­˜å°†å ç”¨å¤§é‡ç£ç›˜ç©ºé—´ã€‚è¿™ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ï¼Œä½†ä½ å¯ä»¥å°†Llama3è®¾ç½®ä¸º4bitï¼Œå¯¹éªŒè¯æŸå¤±å‡ ä¹æ²¡æœ‰å¯æµ‹é‡çš„å½±å“ã€‚\n\nå¦‚æœä¸è¿›è¡Œå—äº¤æ¢ï¼Œä½ å°†éœ€è¦48GBæ˜¾å­˜ï¼Œæˆ–å¸¦æœ‰æµæ°´çº¿å¹¶è¡Œçš„2Ã—24GBã€‚æœ‰è¶³å¤Ÿçš„å—äº¤æ¢ï¼Œä½ å¯ä»¥åœ¨å•ä¸ª24GB GPUä¸Šè®­ç»ƒã€‚ä½¿ç”¨nf4é‡åŒ–ä¹Ÿå…è®¸åœ¨24GBæ˜¾å­˜ä¸‹è®­ç»ƒï¼Œä½†å¯èƒ½ä¼šæœ‰ä¸€äº›è´¨é‡ä¸‹é™ã€‚\n\nHiDreamçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## Stable Diffusion 3\n```\n[model]\ntype = 'sd3'\ndiffusers_path = '/data2/imagegen_models/stable-diffusion-3.5-medium'\ndtype = 'bfloat16'\n#transformer_dtype = 'float8'\n#flux_shift = true\n```\n\næ”¯æŒStable Diffusion 3çš„LoRAè®­ç»ƒã€‚ä½ éœ€è¦æ¨¡å‹çš„å®Œæ•´Diffusersæ–‡ä»¶å¤¹ã€‚å·²åœ¨SD3.5 Mediumå’ŒLargeä¸Šæµ‹è¯•ã€‚\n\nSD3çš„LoRAä»¥Diffusersæ ¼å¼ä¿å­˜ã€‚æ­¤æ ¼å¼å¯åœ¨ComfyUIä¸­ä½¿ç”¨ã€‚\n\n## Cosmos-Predict2\n```\n[model]\ntype = 'cosmos_predict2'\ntransformer_path = '/data2/imagegen_models/Cosmos-Predict2-2B-Text2Image/model.pt'\nvae_path = '/data2/imagegen_models/comfyui-models/wan_2.1_vae.safetensors'\nt5_path = '/data2/imagegen_models/comfyui-models/oldt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\n#transformer_dtype = 'float8_e5m2'\n```\n\nCosmos-Predict2æ”¯æŒLoRAå’Œå…¨é‡å¾®è°ƒã€‚ç›®å‰ä»…æ”¯æŒt2iæ¨¡å‹å˜ä½“ã€‚\n\nå°†transformer_pathè®¾ç½®ä¸ºåŸå§‹æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œvae_pathè®¾ç½®ä¸ºComfyUI Wan VAEï¼Œt5_pathè®¾ç½®ä¸ºComfyUI [æ—§ç‰ˆT5æ¨¡å‹æ–‡ä»¶](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/blob/main/text_encoders/oldt5_xxl_fp16.safetensors)ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯è¾ƒæ—§ç‰ˆæœ¬çš„T5ï¼Œä¸æ˜¯ä¸å…¶ä»–æ¨¡å‹æ›´å¸¸ç”¨çš„ç‰ˆæœ¬ã€‚\n\nè¯¥æ¨¡å‹ä¼¼ä¹æ¯”å¤§å¤šæ•°æ¨¡å‹å¯¹fp8/é‡åŒ–æ›´æ•æ„Ÿã€‚float8_e4m3fn**ä¸ä¼š**å¾ˆå¥½åœ°å·¥ä½œã€‚å¦‚æœä½ ä½¿ç”¨fp8 transformerï¼Œè¯·ä½¿ç”¨é…ç½®ä¸­æ‰€ç¤ºçš„float8_e5m2ã€‚å¦‚æœå¯èƒ½ï¼Œå°½é‡é¿å…åœ¨2Bæ¨¡å‹ä¸Šä½¿ç”¨fp8ã€‚åœ¨14B transformerä¸Šä½¿ç”¨float8_e5m2ä¼¼ä¹æ²¡é—®é¢˜ï¼Œå¹¶ä¸”æ˜¯åœ¨24GB GPUä¸Šè®­ç»ƒæ‰€å¿…éœ€çš„ã€‚\n\nfloat8_e5m2ä¹Ÿæ˜¯ç›®å‰ï¼ˆæ’°å†™æœ¬æ–‡æ—¶ï¼‰å”¯ä¸€å¯ç”¨äºæ¨ç†çš„æ•°æ®ç±»å‹ã€‚ä½†è¯·æ³¨æ„ï¼Œåœ¨ComfyUIä¸­ï¼Œ**å½“åº”ç”¨äºfloat8_e5m2æ¨¡å‹æ—¶ï¼ŒLoRAä¸èƒ½å¾ˆå¥½åœ°å·¥ä½œ**ã€‚ç”Ÿæˆçš„å›¾åƒéå¸¸å˜ˆæ‚ã€‚æˆ‘çŒœåœ¨å°†LoRAæƒé‡ä¸æ­¤æ•°æ®ç±»å‹åˆå¹¶æ—¶çš„éšæœºèˆå…¥ä¼šå¼•å…¥å¤ªå¤šå™ªå£°ã€‚æ­¤é—®é¢˜ä¸å½±å“è®­ç»ƒï¼Œå› ä¸ºLoRAæƒé‡æ˜¯åˆ†ç¦»çš„ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¸ä¼šåˆå¹¶ã€‚ç®€è€Œè¨€ä¹‹ï¼šä½ å¯ä»¥ä½¿ç”¨```transformer_dtype = 'float8_e5m2'```æ¥ä¸º14Bè®­ç»ƒLoRAï¼Œä½†åœ¨ComfyUIä¸­åº”ç”¨LoRAæ—¶ä¸è¦åœ¨æ­¤æ¨¡å‹ä¸Šä½¿ç”¨fp8ã€‚æ›´æ–°ï¼šä½¿ç”¨GGUFæ¨¡å‹æƒé‡æ—¶ï¼ŒLoRAå°†æ­£å¸¸å·¥ä½œï¼Œå› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒLoRAä¸ä¼šåˆå¹¶åˆ°é‡åŒ–æƒé‡ä¸­ã€‚\n\nCosmos-Predict2çš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## OmniGen2\n```\n[model]\ntype = 'omnigen2'\ndiffusers_path = '/data2/imagegen_models/OmniGen2'\ndtype = 'bfloat16'\n#flux_shift = true\n```\n\næ”¯æŒOmniGen2çš„LoRAè®­ç»ƒã€‚å°†```diffusers_path```è®¾ç½®ä¸ºåŸå§‹æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•ã€‚ä»…æ”¯æŒt2iè®­ç»ƒï¼ˆå³å•å¼ å›¾åƒå’Œæ ‡é¢˜ï¼‰ã€‚\n\nOmniGen2çš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n## Flux Kontext\n```\n[model]\ntype = 'flux'\n# æˆ–è€…ç›´æ¥æŒ‡å‘Flux Kontext Diffusersæ–‡ä»¶å¤¹ï¼Œæ— éœ€transformer_path\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\ntransformer_path = '/data2/imagegen_models/flux-dev-single-files/flux1-kontext-dev.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n#flux_shift = true\n```\n\næ”¯æŒFlux Kontextï¼Œé€‚ç”¨äºæ ‡å‡†t2iæ•°æ®é›†å’Œç¼–è¾‘æ•°æ®é›†ã€‚æƒé‡å½¢çŠ¶ä¸Flux Dev 100%å…¼å®¹ï¼Œå› æ­¤å¦‚æœä½ å·²ç»æœ‰Dev Diffusersæ–‡ä»¶å¤¹ï¼Œå¯ä»¥ä½¿ç”¨transformer_pathæŒ‡å‘Kontextå•ä¸€æ¨¡å‹æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´ã€‚\n\nå‚è§[Flux Kontextç¤ºä¾‹æ•°æ®é›†é…ç½®](../examples/flux_kontext_dataset.toml)äº†è§£å¦‚ä½•é…ç½®æ•°æ®é›†ã€‚\n\n**é‡è¦**ï¼šæ§åˆ¶/ä¸Šä¸‹æ–‡å›¾åƒçš„çºµæ¨ªæ¯”åº”ä¸ç›®æ ‡å›¾åƒå¤§è‡´ç›¸åŒã€‚æ‰€æœ‰çºµæ¨ªæ¯”å’Œå¤§å°åˆ†æ¡¶éƒ½æ˜¯é’ˆå¯¹ç›®æ ‡å›¾åƒè¿›è¡Œçš„ã€‚ç„¶åï¼Œæ§åˆ¶å›¾åƒä¼šè¢«è°ƒæ•´å¤§å°å¹¶è£å‰ªä»¥åŒ¹é…ç›®æ ‡å›¾åƒå¤§å°ã€‚å¦‚æœæ§åˆ¶å›¾åƒçš„çºµæ¨ªæ¯”ä¸ç›®æ ‡å›¾åƒå·®å¼‚å¾ˆå¤§ï¼Œå°†ä¼šè£å‰ªæ‰æ§åˆ¶å›¾åƒçš„å¾ˆå¤šéƒ¨åˆ†ã€‚\n\nFlux Kontextçš„LoRAä»¥Diffusersæ ¼å¼ä¿å­˜ï¼Œå¯åœ¨ComfyUIä¸­ä½¿ç”¨ã€‚\n\n## Wan2.2\nä»æ£€æŸ¥ç‚¹åŠ è½½ï¼š\n```\n[model]\ntype = 'wan'\nckpt_path = '/data/imagegen_models/Wan2.2-T2V-A14B'\ntransformer_path = '/data/imagegen_models/Wan2.2-T2V-A14B/low_noise_model'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\nmin_t = 0\nmax_t = 0.875\n```\næˆ–è€…ï¼Œä»ComfyUIæ–‡ä»¶åŠ è½½ä»¥èŠ‚çœç©ºé—´ï¼š\n```\n[model]\ntype = 'wan'\nckpt_path = '/data/imagegen_models/Wan2.2-T2V-A14B'\ntransformer_path = '/data/imagegen_models/comfyui-models/wan2.2_t2v_low_noise_14B_fp16.safetensors'\nllm_path = '/data2/imagegen_models/comfyui-models/umt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n```\n\n5Bæ¨¡å‹ä¹Ÿå—æ”¯æŒï¼Œä½†ä»…ç”¨äºt2v/t2iè®­ç»ƒï¼Œä¸æ”¯æŒi2vã€‚\n\nLoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n### æ¨¡å‹åŠ è½½è¯´æ˜\nä»ComfyUIæ–‡ä»¶åŠ è½½æ—¶ï¼Œä½ ä»ç„¶éœ€è¦åŒ…å«VAEå’Œé…ç½®æ–‡ä»¶çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ï¼Œä½†å®ƒä¸éœ€è¦transformeræˆ–T5ã€‚ä½ å¯ä»¥åƒè¿™æ ·ä¸‹è½½å¹¶è·³è¿‡è¿™äº›æ–‡ä»¶ï¼š\n```\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir Wan2.2-T2V-A14B --exclude \"models_t5*\" \"*/diffusion_pytorch_model*\"\n```\nå¯¹äºWan2.2 A14Bï¼Œå¦‚æœä½ å®Œå…¨ä»æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹åŠ è½½ï¼Œéœ€è¦ä½¿ç”¨```transformer_path```æŒ‡å‘ä½ æƒ³è¦è®­ç»ƒçš„æ¨¡å‹å­æ–‡ä»¶å¤¹ï¼Œå³ä½å™ªå£°æˆ–é«˜å™ªå£°ã€‚\n\n### æ—¶é—´æ­¥èŒƒå›´\nWan2.2 A14Bæœ‰ä¸¤ä¸ªæ¨¡å‹ï¼šä½å™ªå£°å’Œé«˜å™ªå£°ã€‚å®ƒä»¬åœ¨æ¨ç†æœŸé—´å¤„ç†æ—¶é—´æ­¥èŒƒå›´çš„ä¸åŒéƒ¨åˆ†ï¼Œå½“æ—¶é—´æ­¥è¾¾åˆ°æŸä¸ªè¾¹ç•Œæ—¶åœ¨æ¨¡å‹ä¹‹é—´åˆ‡æ¢ã€‚t=0è¡¨ç¤ºæ— å™ªå£°ï¼Œt=1è¡¨ç¤ºå®Œå…¨å™ªå£°ã€‚è¿™äº›æ¨¡å‹æ˜¯ç‹¬ç«‹çš„ï¼›ä½ å¯ä»¥ä¸ºå…¶ä¸­ä¸€ä¸ªæˆ–ä¸¤ä¸ªè®­ç»ƒLoRAã€‚\n\næˆ‘æ‰¾ä¸åˆ°Wanå›¢é˜Ÿä¸ºæ¯ä¸ªæ¨¡å‹è®­ç»ƒçš„ç¡®åˆ‡æ—¶é—´æ­¥ç»†èŠ‚ï¼Œä½†æ¨æµ‹ä»–ä»¬æ˜¯æŒ‰ç…§æ¨ç†æ—¶çš„ä½¿ç”¨æ–¹å¼è¿›è¡Œè®­ç»ƒçš„ã€‚å¯¹äºT2Væ¨¡å‹ï¼Œé…ç½®çš„æ¨ç†è¾¹ç•Œæ—¶é—´æ­¥ä¸º0.875ã€‚å¯¹äºI2Vï¼Œå®ƒæ˜¯0.9ã€‚ä½ å¯ä»¥ï¼ˆå¹¶ä¸”åº”è¯¥ï¼‰ä½¿ç”¨```min_t```å’Œ```max_t```å‚æ•°æ¥é™åˆ¶é€‚åˆæ¨¡å‹çš„è®­ç»ƒæ—¶é—´æ­¥èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œä¸Šé¢çš„ç¬¬ä¸€ä¸ªæ¨¡å‹é…ç½®è®¾ç½®äº†ä½å™ªå£°T2Væ¨¡å‹çš„æ—¶é—´æ­¥èŒƒå›´ã€‚æˆ‘ä¸çŸ¥é“è®­ç»ƒæ—¶é—´æ­¥èŒƒå›´æ˜¯å¦åº”è¯¥ä¸æ¨ç†è¾¹ç•Œå®Œå…¨åŒ¹é…ã€‚å¯¹äºé«˜å™ªå£°T2Væ¨¡å‹ï¼Œä½ å°†ä½¿ç”¨ï¼š\n```\nmin_t = 0.875\nmax_t = 1\n```\nåƒè¿™æ ·æ§åˆ¶æ—¶é—´æ­¥èŒƒå›´ï¼Œå³ä½¿ä½ ä½¿ç”¨```shift```æˆ–```flux_shift```å‚æ•°æ¥åç§»æ—¶é—´æ­¥åˆ†å¸ƒï¼Œä¹Ÿèƒ½æ­£å¸¸å·¥ä½œã€‚\n\næˆ–è€…ï¼Œäººä»¬æ³¨æ„åˆ°ä½å™ªå£°æ¨¡å‹å¯ä»¥å•ç‹¬ä½¿ç”¨ã€‚å› æ­¤ï¼Œä½ å¯ä»¥åƒè®­ç»ƒWan2.1ä¸€æ ·è®­ç»ƒä½å™ªå£°æ¨¡å‹ï¼Œè€Œæ— éœ€é™åˆ¶æ—¶é—´æ­¥èŒƒå›´ã€‚\n\n## Qwen-Image\n```\n[model]\ntype = 'qwen_image'\ndiffusers_path = '/data/imagegen_models/Qwen-Image'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\næˆ–ä»å•ä¸ªæ–‡ä»¶åŠ è½½ï¼š\n```\n[model]\ntype = 'qwen_image'\ntransformer_path = '/data/imagegen_models/comfyui-models/qwen_image_bf16.safetensors'\ntext_encoder_path = '/data/imagegen_models/comfyui-models/qwen_2.5_vl_7b.safetensors'\nvae_path = '/data/imagegen_models/Qwen-Image/vae/diffusion_pytorch_model.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nåœ¨ç¬¬äºŒç§æ ¼å¼ä¸­ï¼Œ```transformer_path```å’Œ```text_encoder_path```åº”è¯¥æ˜¯ComfyUIæ–‡ä»¶ï¼Œä½†```vae_path```éœ€è¦æ˜¯**Diffusers VAE**ï¼ˆæƒé‡é”®åå®Œå…¨ä¸åŒï¼ŒComfyUI VAEç›®å‰ä¸æ”¯æŒï¼‰ã€‚å³ä½¿ä½ å°†transformerè½¬æ¢ä¸ºfloat8ï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨bf16æ–‡ä»¶ï¼›fp8_scaledæƒé‡æ ¹æœ¬æ— æ³•å·¥ä½œï¼Œfp8æƒé‡å¯èƒ½è´¨é‡ç¨ä½ï¼Œå› ä¸ºè®­ç»ƒè„šæœ¬å°è¯•å°†ä¸€äº›æƒé‡ä¿æŒåœ¨æ›´é«˜ç²¾åº¦ã€‚å¦‚æœä½ åŒæ—¶æä¾›```diffusers_path```å’Œå„ä¸ªæ¨¡å‹è·¯å¾„ï¼Œå®ƒå°†ä¼˜å…ˆä»å•ä¸ªè·¯å¾„è¯»å–å­æ¨¡å‹ã€‚\n\nåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œä½ éœ€è¦æœ€æ–°çš„Diffusersï¼š\n```\npip uninstall diffusers\npip install git+https://github.com/huggingface/diffusers\n```\n\nQwen-Imageçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n### åœ¨å•ä¸ª24GB GPUä¸Šè®­ç»ƒLoRA\n- ä½ å°†éœ€è¦å—äº¤æ¢ã€‚å‚è§[ç¤ºä¾‹24GBæ˜¾å­˜é…ç½®](../examples/qwen_image_24gb_vram.toml)ï¼Œå…¶ä¸­æ‰€æœ‰è®¾ç½®éƒ½æ˜¯æ­£ç¡®çš„ã€‚\n- ä½¿ç”¨å¯æ‰©å±•æ®µCUDAåŠŸèƒ½ï¼š```PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" deepspeed --num_gpus=1 train.py --deepspeed --config /home/anon/code/diffusion-pipe-configs/tmp.toml```\n- ä½¿ç”¨640çš„æ•°æ®é›†åˆ†è¾¨ç‡ã€‚è¿™æ˜¯æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ†è¾¨ç‡ä¹‹ä¸€ï¼Œå¯èƒ½æ¯”512ç¨å¥½ã€‚\n- å¦‚æœä½ ä½¿ç”¨æ›´é«˜çš„LoRAç§©æˆ–æ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œå¯èƒ½éœ€è¦å¢åŠ blocks_to_swapã€‚\n\n## Qwen-Image-Edit\n```\n[model]\ntype = 'qwen_image'\ndiffusers_path = '/data/imagegen_models/Qwen-Image'  # æˆ–è€…ï¼ŒQwen-Image-Edit Diffusersæ–‡ä»¶å¤¹\n# ä»…å½“ä½ ä½¿ç”¨Qwen-Image Diffusersæ¨¡å‹è€Œä¸æ˜¯Qwen-Image-Editæ—¶éœ€è¦\ntransformer_path = '/data/imagegen_models/comfyui-models/qwen_image_edit_bf16.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\né…ç½®å’Œè®­ç»ƒQwen-Image-Editä¸Flux-Kontextç›¸åŒã€‚å‚è§[ç¤ºä¾‹æ•°æ®é›†é…ç½®](../examples/flux_kontext_dataset.toml)ã€‚åŒæ ·çš„æ•°æ®é›†æ³¨æ„äº‹é¡¹é€‚ç”¨ã€‚å‚è€ƒå›¾åƒä¼šè¢«è°ƒæ•´å¤§å°åˆ°ç›®æ ‡å›¾åƒæœ€ç»ˆæ‰€åœ¨çš„ä»»ä½•å¤§å°åˆ†æ¡¶ï¼Œå› æ­¤ä½ çš„å‚è€ƒå›¾åƒéœ€è¦ä¸ç›®æ ‡å›¾åƒå…·æœ‰å¤§è‡´ç›¸åŒçš„çºµæ¨ªæ¯”ï¼Œå¦åˆ™å®ƒä»¬ä¼šè¢«è¿‡åº¦è£å‰ªã€‚\n\nè¯¥æ¨¡å‹æ¥å—çš„è¾“å…¥æ¯”T2Iè®­ç»ƒæ›´å¤§ï¼Œå› æ­¤é€Ÿåº¦æ›´æ…¢ï¼Œä½¿ç”¨æ›´å¤šæ˜¾å­˜ã€‚æˆ‘ä¸çŸ¥é“æ˜¯å¦å¯ä»¥åœ¨24GBæ˜¾å­˜ä¸Šè®­ç»ƒå®ƒã€‚ä¹Ÿè®¸å¦‚æœè¿›è¡Œè¶³å¤Ÿçš„å—äº¤æ¢ã€‚\n\nQwen-Image-Editçš„LoRAä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚\n\n\n\n# HunyuanImage-2.1ï¼ˆæ··å…ƒå›¾åƒæ¨¡å‹2.1ï¼‰\nä½¿ç”¨å…¼å®¹ComfyUIçš„æ¨¡å‹æ–‡ä»¶ã€‚\n```\n[model]\ntype = 'hunyuan_image'  # æ¨¡å‹ç±»å‹ = 'æ··å…ƒå›¾åƒæ¨¡å‹'\ntransformer_path = '/data/imagegen_models/comfyui-models/hunyuanimage2.1.safetensors'  # Transformeræ¨¡å—è·¯å¾„\nvae_path = '/data/imagegen_models/comfyui-models/hunyuan_image_2.1_vae_fp16.safetensors'  # VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ¨¡å—è·¯å¾„\ntext_encoder_path = '/data/imagegen_models/comfyui-models/qwen_2.5_vl_7b.safetensors'  # æ–‡æœ¬ç¼–ç å™¨è·¯å¾„\nbyt5_path = '/data/imagegen_models/comfyui-models/byt5_small_glyphxl_fp16.safetensors'  # ByT5ï¼ˆå­—èŠ‚çº§Transformeræ¨¡å‹ï¼‰è·¯å¾„\ndtype = 'bfloat16'  # æ•°æ®ç±»å‹ = '16ä½è„‘æµ®ç‚¹æ•°'\ntransformer_dtype = 'float8'  # Transformeræ¨¡å—æ•°æ®ç±»å‹ = '8ä½æµ®ç‚¹æ•°'\n```\n\n## å…³äºå›¾åƒåˆ†è¾¨ç‡çš„è¯´æ˜\nç”±äºVAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰çš„é«˜ç©ºé—´å‹ç¼©ç‰¹æ€§ä»¥åŠDiTï¼ˆæ‰©æ•£Transformerï¼‰æ¨¡å‹çš„æ¶æ„è®¾è®¡ï¼Œåœ¨ç‰¹å®šå›¾åƒåˆ†è¾¨ç‡ä¸‹ï¼ŒHunyuanImage-2.1çš„è®¡ç®—é‡å’Œå†…å­˜éœ€æ±‚ï¼Œä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚Fluxã€Qwenã€Luminaç­‰ï¼‰åœ¨**ä¸€åŠå›¾åƒè¾¹é•¿åˆ†è¾¨ç‡**ä¸‹çš„éœ€æ±‚ç›¸åŒã€‚  \nä¹Ÿå°±æ˜¯è¯´ï¼ŒHunyuanImage-2.1åœ¨1024åˆ†è¾¨ç‡ä¸‹çš„è®¡ç®—é‡ï¼Œç­‰åŒäºFluxã€Qwenã€Luminaç­‰æ¨¡å‹åœ¨512åˆ†è¾¨ç‡ä¸‹çš„è®¡ç®—é‡ã€‚  \n\nä½ å¯ä»¥é€‰æ‹©åœ¨512åˆ†è¾¨ç‡ä¸‹è¿›è¡Œè®­ç»ƒä»¥æå‡é€Ÿåº¦â€”â€”å®è·µè¡¨æ˜ï¼Œå³ä¾¿è¯¥åˆ†è¾¨ç‡å¯¹äºHunyuanImage-2.1è€Œè¨€ç›¸å¯¹è¾ƒä½ï¼Œæ¨¡å‹ä»èƒ½è¾ƒå¥½åœ°å®Œæˆå­¦ä¹ ã€‚ä½†æ ¹æ®æ•°æ®é›†çš„ä¸åŒï¼Œå¯èƒ½æ›´é€‚åˆåœ¨1024åŠä»¥ä¸Šåˆ†è¾¨ç‡ä¸‹è®­ç»ƒï¼Œå°¤å…¶æ˜¯å½“ä½ å¸Œæœ›ä»æ•°æ®é›†ä¸­å­¦ä¹ åˆ°ç‹¬ç‰¹çš„ç»†ç²’åº¦ç»†èŠ‚æ—¶ã€‚\n\nHunyuanImage-2.1çš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰æ¨¡å‹ä»¥ComfyUIæ ¼å¼ä¿å­˜ã€‚éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œè¿™æ„å‘³ç€å…¶éƒ¨åˆ†é”®åï¼ˆkey namesï¼‰ä¸åŸå§‹æ¨¡å‹ç»“æ„å­˜åœ¨å·®å¼‚ã€‚å¦‚æœä½ è®¡åˆ’åœ¨ComfyUIä»¥å¤–çš„å¹³å°ä½¿ç”¨è¯¥LoRAæ¨¡å‹ï¼Œè¯·åŠ¡å¿…ç•™æ„è¿™ä¸€ç‚¹ã€‚\n"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 229,
      "type": "OutputDirPassthrough",
      "pos": [
        -1900,
        -200
      ],
      "size": [
        224.31991577148438,
        26
      ],
      "flags": {},
      "order": 35,
      "mode": 0,
      "inputs": [
        {
          "name": "output_dir",
          "type": "STRING",
          "link": 436
        }
      ],
      "outputs": [
        {
          "name": "output_dir",
          "type": "STRING",
          "links": [
            328
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "OutputDirPassthrough",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 147,
      "type": "SD3ModelNode",
      "pos": [
        -3514.805419921875,
        -2782.400390625
      ],
      "size": [
        390.72479248046875,
        82
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "SD3ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "",
        true
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 150,
      "type": "OmniGen2ModelNode",
      "pos": [
        -3446.3203125,
        -2479.175048828125
      ],
      "size": [
        330,
        82
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "OmniGen2ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        true
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 274,
      "type": "HunyuanImage21ModelNode",
      "pos": [
        -3433.848388671875,
        -3183.912109375
      ],
      "size": [
        406.7791748046875,
        146.21170043945312
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "9847d6ead95f7a46fc66aa3ed090f0fa1b9078ba",
        "Node name for S&R": "HunyuanImage21ModelNode",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "1",
        "4",
        "3",
        "2"
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 151,
      "type": "FluxKontextModelNode",
      "pos": [
        -2171.85302734375,
        -2380.76611328125
      ],
      "size": [
        396.4298400878906,
        106
      ],
      "flags": {},
      "order": 12,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "FluxKontextModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        "",
        false
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 135,
      "type": "HunyuanVideoModelNode",
      "pos": [
        -2946.67724609375,
        -3189.364013671875
      ],
      "size": [
        713.2388916015625,
        154
      ],
      "flags": {},
      "order": 13,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "HunyuanVideoModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Z:\\home\\ly\\comfy\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI\\Qwen-Image\\transformer",
        "Z:\\home\\ly\\comfy\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI\\Qwen-Image\\transformer",
        "Z:\\home\\ly\\comfy\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI\\Qwen-Image\\transformer",
        "Z:\\home\\ly\\comfy\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI\\Qwen-Image\\transformer",
        "Z:\\home\\ly\\comfy\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI\\Qwen-Image\\transformer"
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 138,
      "type": "CosmosModelNode",
      "pos": [
        -2191.632568359375,
        -3193.61865234375
      ],
      "size": [
        538.4299926757812,
        106
      ],
      "flags": {},
      "order": 14,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "CosmosModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        "sdxl_vae.safetensors",
        "umt5-xxl-enc-bf16.safetensors"
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 139,
      "type": "Lumina2ModelNode",
      "pos": [
        -2121.632568359375,
        -3033.61865234375
      ],
      "size": [
        421.02410888671875,
        130
      ],
      "flags": {},
      "order": 15,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "Lumina2ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        "chatglm3-4bit.safetensors",
        "vae-ft-mse-840000-ema-pruned.safetensors",
        true
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 180,
      "type": "PreviewAny",
      "pos": [
        -1339.8380126953125,
        -1199.9976806640625
      ],
      "size": [
        537.2894287109375,
        652.601318359375
      ],
      "flags": {},
      "order": 38,
      "mode": 0,
      "inputs": [
        {
          "name": "source",
          "type": "*",
          "link": 280
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.57",
        "Node name for S&R": "PreviewAny",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 141,
      "type": "Wan21ModelNode",
      "pos": [
        -2837.1748046875,
        -2708.0126953125
      ],
      "size": [
        670,
        106
      ],
      "flags": {},
      "order": 16,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "Wan21ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate",
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\transformers\\TencentGameMate"
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 152,
      "type": "Wan22ModelNode",
      "pos": [
        -2856.6484375,
        -2455.501708984375
      ],
      "size": [
        670,
        154
      ],
      "flags": {},
      "order": 17,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "Wan22ModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "",
        0,
        1,
        "",
        ""
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 281,
      "type": "SDXLModelNode",
      "pos": [
        -3465.358642578125,
        -2117.285888671875
      ],
      "size": [
        403.1981201171875,
        202
      ],
      "flags": {},
      "order": 18,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "fec13b6b9bd0a24ec8f9902006871c34a2b4a018",
        "Node name for S&R": "SDXLModelNode",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "1",
        false,
        0,
        false,
        0.00004,
        0.00002,
        0.00002
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 279,
      "type": "easy showAnything",
      "pos": [
        -2813.48486328125,
        -1477.53466796875
      ],
      "size": [
        539.8226318359375,
        169.6727294921875
      ],
      "flags": {},
      "order": 30,
      "mode": 0,
      "inputs": [
        {
          "label": "è¾“å…¥ä»»ä½•",
          "name": "anything",
          "shape": 7,
          "type": "*",
          "link": 395
        }
      ],
      "outputs": [
        {
          "name": "output",
          "type": "*",
          "links": null
        }
      ],
      "properties": {
        "cnr_id": "comfyui-easy-use",
        "ver": "f550ce83a84114b0eeae5510679910e914f5d514",
        "Node name for S&R": "easy showAnything",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "{\"type\": \"qwen_image\", \"diffusers_path\": \"E:\\\\comfyuiMQ\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\models\\\\diffusers\\\\Qwen-Image\"}"
      ]
    },
    {
      "id": 278,
      "type": "FluxModelNode",
      "pos": [
        -3027.3740234375,
        -2216.519287109375
      ],
      "size": [
        493.0029602050781,
        130
      ],
      "flags": {},
      "order": 19,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "0f0da5e557e65a78bc05937db748cf76ee19d0e1",
        "Node name for S&R": "FluxModelNode",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Z:\\home\\TIANDONG\\comfy\\ComfyUI\\models\\unet\\FLUX1-dev",
        "",
        true,
        true
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 155,
      "type": "QwenImageEditModelNode",
      "pos": [
        -2499.26708984375,
        -2227.069091796875
      ],
      "size": [
        440,
        82
      ],
      "flags": {},
      "order": 20,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": []
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "QwenImageEditModelNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Qwen-Image",
        ""
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 188,
      "type": "PreviewAny",
      "pos": [
        -1910,
        -1080
      ],
      "size": [
        518.5387573242188,
        595.182373046875
      ],
      "flags": {
        "collapsed": false
      },
      "order": 39,
      "mode": 0,
      "inputs": [
        {
          "name": "source",
          "type": "*",
          "link": 382
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.57",
        "Node name for S&R": "PreviewAny",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 283,
      "type": "ShowText|pysssss",
      "pos": [
        -1300,
        30
      ],
      "size": [
        400,
        200
      ],
      "flags": {},
      "order": 41,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 398
        }
      ],
      "outputs": [
        {
          "label": "å­—ç¬¦ä¸²",
          "name": "STRING",
          "shape": 6,
          "type": "STRING",
          "links": null
        }
      ],
      "properties": {
        "cnr_id": "comfyui-custom-scripts",
        "ver": "f2838ed5e59de4d73cde5c98354b87a8d3200190",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "ğŸŸ¢ è¿è¡Œä¸­ (Running) (PID: 22140)\nâ±ï¸  è¿è¡Œæ—¶é—´ (Runtime): 00:00:05\nğŸ“ ç›‘æ§ç›®å½• (Monitor Dir): ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI_Win\\output\\123\\20251007_18-01-24\nğŸŒ è®¿é—®åœ°å€ (Access URL): http://localhost:4666\nğŸ“Š å‘ç° 1 ä¸ªäº‹ä»¶æ–‡ä»¶ (Found 1 event files):\n   â€¢ events.out.tfevents.1759860098.BEIMENPANYAN.22376.0"
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 193,
      "type": "QwenImageModelNode",
      "pos": [
        -3507.55712890625,
        -1327.5853271484375
      ],
      "size": [
        610,
        154
      ],
      "flags": {},
      "order": 21,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "links": [
            394,
            395
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "QwenImageModelNode",
        "ue_properties": {
          "widget_ue_connectable": {
            "diffusers_path": true,
            "transformer_path": true,
            "text_encoder_path": true,
            "tokenizer_path": true,
            "vae_path": true
          },
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusers\\Qwen-Image",
        "",
        "",
        "",
        ""
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 175,
      "type": "AdapterConfigNode",
      "pos": [
        -3380,
        -780
      ],
      "size": [
        256.1302795410156,
        130
      ],
      "flags": {},
      "order": 22,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "adapter_config",
          "type": "ADAPTER_CONFIG",
          "links": [
            432
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "AdapterConfigNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "lora",
        16,
        "bfloat16",
        ""
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 282,
      "type": "AdvancedTrainConfig",
      "pos": [
        -3347.61376953125,
        -599.2967529296875
      ],
      "size": [
        349.455078125,
        346
      ],
      "flags": {},
      "order": 23,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "advanced_config",
          "type": "ADVANCED_TRAIN_CONFIG",
          "links": [
            433
          ]
        }
      ],
      "properties": {
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "fabac77e6c3101d3c76c75b770281de4b0ac3edd",
        "Node name for S&R": "AdvancedTrainConfig",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        0,
        0,
        "constant",
        0,
        0,
        false,
        1,
        false,
        0,
        0,
        1,
        "",
        false
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 231,
      "type": "PrimitiveString",
      "pos": [
        -1870,
        40
      ],
      "size": [
        497.604736328125,
        58
      ],
      "flags": {},
      "order": 24,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": []
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "PrimitiveString",
        "ue_properties": {
          "widget_ue_connectable": {
            "value": true
          },
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "Z:\\root\\wan2_2_lora\\diffusion-pipe\\training_runs\\qwen_edit"
      ],
      "color": "#2a363b",
      "bgcolor": "#3f5159"
    },
    {
      "id": 236,
      "type": "Note",
      "pos": [
        -1620,
        -340
      ],
      "size": [
        329.5334777832031,
        88
      ],
      "flags": {},
      "order": 25,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "do not delete this"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 75,
      "type": "GeneralDatasetPathNode",
      "pos": [
        -4999.1337890625,
        -1180.0953369140625
      ],
      "size": [
        720,
        58
      ],
      "flags": {},
      "order": 26,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "input_path",
          "type": "input_path",
          "links": [
            338
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "GeneralDatasetPathNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI_Win\\input\\test"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 176,
      "type": "ModelConfig",
      "pos": [
        -2873.414306640625,
        -1232.0546875
      ],
      "size": [
        320,
        106
      ],
      "flags": {},
      "order": 29,
      "mode": 0,
      "inputs": [
        {
          "name": "model_path",
          "type": "model_path",
          "link": 394
        }
      ],
      "outputs": [
        {
          "name": "model_config",
          "type": "model_config",
          "links": [
            430
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "ModelConfig",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "bfloat16",
        "float8",
        "logit_normal"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 291,
      "type": "easy showAnything",
      "pos": [
        -4230,
        -750
      ],
      "size": [
        600,
        390
      ],
      "flags": {},
      "order": 32,
      "mode": 0,
      "inputs": [
        {
          "label": "è¾“å…¥ä»»ä½•",
          "name": "anything",
          "shape": 7,
          "type": "*",
          "link": 428
        }
      ],
      "outputs": [
        {
          "name": "output",
          "type": "*",
          "links": null
        }
      ],
      "properties": {
        "cnr_id": "comfyui-easy-use",
        "ver": "b6deb5f5155fd400b20cce7f8644a61efdcbe098",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "resolutions = [512, 720]\nenable_ar_bucket = true\nmin_ar = 0.5\nmax_ar = 2.0\nnum_ar_buckets = 7\nar_buckets = [[512, 512], [448, 576], [1024, 1024]]\n[[directory]]\npath = 'E:\\comfyuiMQ\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\Diffusion_pipe_in_ComfyUI_Win\\input\\test'\nnum_repeats = 5"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 292,
      "type": "GeneralConfig",
      "pos": [
        -2840,
        -1050
      ],
      "size": [
        700,
        672
      ],
      "flags": {},
      "order": 33,
      "mode": 0,
      "inputs": [
        {
          "name": "optimizer_config",
          "type": "OPTIMIZER_CONFIG",
          "link": 429
        },
        {
          "name": "model_config",
          "type": "model_config",
          "link": 430
        },
        {
          "name": "dataset_config",
          "type": "DATASET_CONFIG",
          "link": 431
        },
        {
          "name": "adapter_config",
          "shape": 7,
          "type": "ADAPTER_CONFIG",
          "link": 432
        },
        {
          "name": "advanced_config",
          "shape": 7,
          "type": "ADVANCED_TRAIN_CONFIG",
          "link": 433
        }
      ],
      "outputs": [
        {
          "name": "train_config",
          "type": "TRAIN_CONFIG",
          "links": [
            434,
            435
          ]
        },
        {
          "name": "output_dir",
          "type": "STRING",
          "links": [
            436
          ]
        },
        {
          "name": "config_path",
          "type": "config_path",
          "links": [
            437
          ]
        }
      ],
      "properties": {
        "ue_properties": {
          "widget_ue_connectable": {
            "output_folder_name": true,
            "epochs": true,
            "micro_batch_size_per_gpu": true,
            "number_of_gpus": true,
            "pipeline_stages": true,
            "gradient_accumulation_steps": true,
            "gradient_clipping": true,
            "warmup_steps": true,
            "blocks_to_swap": true,
            "activation_checkpointing": true,
            "save_dtype": true,
            "partition_method": true,
            "eval_every_n_epochs": true,
            "eval_before_first_step": true,
            "eval_micro_batch_size_per_gpu": true,
            "eval_gradient_accumulation_steps": true,
            "save_every_n_epochs": true,
            "checkpoint_every_n_minutes": true,
            "caching_batch_size": true,
            "disable_block_swap_for_eval": true,
            "video_clip_mode": true,
            "eval_datasets": true
          },
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "123",
        38,
        1,
        1,
        1,
        2,
        0,
        20,
        36,
        "bfloat16",
        "bfloat16",
        "parameters",
        0,
        1,
        1,
        1,
        1,
        120,
        1,
        true,
        "none",
        ""
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 174,
      "type": "OptimizerConfigNode",
      "pos": [
        -3461.6201171875,
        -1052.1263427734375
      ],
      "size": [
        390,
        202
      ],
      "flags": {},
      "order": 27,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "optimizer_config",
          "type": "OPTIMIZER_CONFIG",
          "links": [
            429
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "OptimizerConfigNode",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "AdamW8bitKahan",
        0.00002,
        0.9,
        0.99,
        0.01,
        1e-8
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 177,
      "type": "GeneralDatasetConfig",
      "pos": [
        -4390,
        -1040
      ],
      "size": [
        820,
        218
      ],
      "flags": {},
      "order": 31,
      "mode": 0,
      "inputs": [
        {
          "name": "input_path",
          "type": "input_path",
          "link": 338
        },
        {
          "name": "frame_buckets",
          "shape": 7,
          "type": "frame_buckets",
          "link": null
        },
        {
          "name": "ar_buckets",
          "shape": 7,
          "type": "ar_buckets",
          "link": 190
        }
      ],
      "outputs": [
        {
          "name": "dataset_config",
          "type": "DATASET_CONFIG",
          "links": [
            278,
            428,
            431
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "GeneralDatasetConfig",
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "[512,720]",
        true,
        0.5,
        2,
        7,
        5
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 289,
      "type": "easy showAnything",
      "pos": [
        -2840,
        -330
      ],
      "size": [
        800,
        610
      ],
      "flags": {},
      "order": 34,
      "mode": 0,
      "inputs": [
        {
          "label": "è¾“å…¥ä»»ä½•",
          "name": "anything",
          "shape": 7,
          "type": "*",
          "link": 435
        }
      ],
      "outputs": [
        {
          "name": "output",
          "type": "*",
          "links": null
        }
      ],
      "properties": {
        "cnr_id": "comfyui-easy-use",
        "ver": "b6deb5f5155fd400b20cce7f8644a61efdcbe098",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "epochs = 38\nmicro_batch_size_per_gpu = 1\nnumber_of_gpus = 1\npipeline_stages = 1\nwarmup_steps = 20\nblocks_to_swap = 36\nactivation_checkpointing = true\nsave_dtype = 'bfloat16'\ncaching_batch_size = 1\npartition_method = 'parameters'\noutput_dir = 'E:/comfyuiMQ/ComfyUI_windows_portable/ComfyUI/custom_nodes/Diffusion_pipe_in_ComfyUI_Win/output/123'\ndisable_block_swap_for_eval = true\neval_datasets = []\ngradient_accumulation_steps = 2\nsave_every_n_epochs = 1\ncheckpoint_every_n_minutes = 120\ndataset = 'E:/comfyuiMQ/ComfyUI_windows_portable/ComfyUI/custom_nodes/Diffusion_pipe_in_ComfyUI_Win/dataset/dataset.toml'\ncheckpoint_every_n_epochs = 1\n\n[optimizer]\ntype = 'AdamW8bitKahan'\nlr = 2e-5\nbetas = [ 0.9, 0.99,]\nweight_decay = 0.01\ngradient_release = false\n\n[model]\ntype = 'qwen_image'\ndiffusers_path = 'E:/comfyuiMQ/ComfyUI_windows_portable/ComfyUI/models/diffusers/Qwen-Image'\ndtype = 'bfloat16'\ntimestep_sample_method = 'logit_normal'\ntransformer_dtype = 'float8'\n\n[adapter]\ntype = 'lora'\nrank = 16\ndtype = 'bfloat16'\n"
      ]
    },
    {
      "id": 230,
      "type": "TensorBoardMonitor",
      "pos": [
        -1640,
        -200
      ],
      "size": [
        299.5706481933594,
        150
      ],
      "flags": {},
      "order": 37,
      "mode": 0,
      "inputs": [
        {
          "name": "output_dir",
          "type": "STRING",
          "link": 328
        }
      ],
      "outputs": [
        {
          "name": "url",
          "type": "STRING",
          "links": [
            399
          ]
        },
        {
          "name": "status",
          "type": "STRING",
          "links": [
            398
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "TensorBoardMonitor",
        "ue_properties": {
          "widget_ue_connectable": {
            "port": true,
            "host": true,
            "is_new_training": true,
            "action": true
          },
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        4666,
        "localhost",
        false,
        "start"
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 284,
      "type": "ShowText|pysssss",
      "pos": [
        -1220,
        -240
      ],
      "size": [
        310,
        200
      ],
      "flags": {},
      "order": 40,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 399
        }
      ],
      "outputs": [
        {
          "label": "å­—ç¬¦ä¸²",
          "name": "STRING",
          "shape": 6,
          "type": "STRING",
          "links": null
        }
      ],
      "properties": {
        "cnr_id": "comfyui-custom-scripts",
        "ver": "f2838ed5e59de4d73cde5c98354b87a8d3200190",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "http://localhost:4666"
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 220,
      "type": "Train",
      "pos": [
        -1909.81640625,
        -1230.0621337890625
      ],
      "size": [
        274.5047912597656,
        98
      ],
      "flags": {},
      "order": 36,
      "mode": 4,
      "inputs": [
        {
          "name": "dataset_config",
          "type": "DATASET_CONFIG",
          "link": 278
        },
        {
          "name": "train_config",
          "type": "TRAIN_CONFIG",
          "link": 434
        },
        {
          "name": "config_path",
          "type": "config_path",
          "link": 437
        }
      ],
      "outputs": [
        {
          "name": "status",
          "type": "STRING",
          "links": [
            280
          ]
        },
        {
          "name": "log_output",
          "type": "STRING",
          "links": [
            382
          ]
        }
      ],
      "properties": {
        "aux_id": "TianDongL/Diffusion_pipe_in_ComfyUI",
        "cnr_id": "Diffusion_pipe_in_ComfyUI",
        "ver": "3a3b86a2487387147f0146233ab8716bbe3aab0f",
        "Node name for S&R": "Train",
        "ue_properties": {
          "widget_ue_connectable": {},
          "version": "7.1",
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        ""
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 122,
      "type": "MarkdownNote",
      "pos": [
        -4522.56591796875,
        -1973.625244140625
      ],
      "size": [
        880,
        640
      ],
      "flags": {},
      "order": 28,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {
        "ue_properties": {
          "version": "7.1",
          "widget_ue_connectable": {},
          "input_ue_unconnectable": {}
        }
      },
      "widgets_values": [
        "# Summary\n\n| Model          | LoRA | Full Fine Tune | fp8/quantization |\n|----------------|------|----------------|------------------|\n|SDXL            |âœ…    |âœ…              |âŒ                |\n|Flux            |âœ…    |âœ…              |âœ…                |\n|LTX-Video       |âœ…    |âŒ              |âŒ                |\n|HunyuanVideo    |âœ…    |âŒ              |âœ…                |\n|Cosmos          |âœ…    |âŒ              |âŒ                |\n|Lumina Image 2.0|âœ…    |âœ…              |âŒ                |\n|Wan2.1          |âœ…    |âœ…              |âœ…                |\n|Chroma          |âœ…    |âœ…              |âœ…                |\n|HiDream         |âœ…    |âŒ              |âœ…                |\n|SD3             |âœ…    |âŒ              |âœ…                |\n|Cosmos-Predict2 |âœ…    |âœ…              |âœ…                |\n|OmniGen2        |âœ…    |âŒ              |âŒ                |\n|Flux Kontext    |âœ…    |âœ…              |âœ…                |\n|Wan2.2          |âœ…    |âœ…              |âœ…                |\n|Qwen-Image      |âœ…    |âœ…              |âœ…                |\n|Qwen-Image-Edit |âœ…    |âœ…              |âœ…                |\n|HunyuanImage-2.1|âœ…    |âœ…              |âœ…                |\n\n\n## SDXL\n```\n[model]\ntype = 'sdxl'\ncheckpoint_path = '/data2/imagegen_models/sdxl/sd_xl_base_1.0_0.9vae.safetensors'\ndtype = 'bfloat16'\n# You can train v-prediction models (e.g. NoobAI vpred) by setting this option.\n#v_pred = true\n# Min SNR is supported. Same meaning as sd-scripts\n#min_snr_gamma = 5\n# Debiased estimation loss is supported. Same meaning as sd-scripts.\n#debiased_estimation_loss = true\n# You can set separate learning rates for unet and text encoders. If one of these isn't set, the optimizer learning rate will apply.\nunet_lr = 4e-5\ntext_encoder_1_lr = 2e-5\ntext_encoder_2_lr = 2e-5\n```\nUnlike other models, for SDXL the text embeddings are not cached, and the text encoders are trained.\n\nSDXL can be full fine tuned. Just remove the [adapter] table in the config file. You will need 48GB VRAM. 2x24GB GPUs works with pipeline_stages=2.\n\nSDXL LoRAs are saved in Kohya sd-scripts format. SDXL full fine tune models are saved in the original SDXL checkpoint format.\n\n## Flux\n```\n[model]\ntype = 'flux'\n# Path to Huggingface Diffusers directory for Flux\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\n# You can override the transformer from a BFL format checkpoint.\n#transformer_path = '/data2/imagegen_models/flux-dev-single-files/consolidated_s6700-schnell.safetensors'\ndtype = 'bfloat16'\n# Flux supports fp8 for the transformer when training LoRA.\ntransformer_dtype = 'float8'  \n# Resolution-dependent timestep shift towards more noise. Same meaning as sd-scripts.\nflux_shift = true\n# For FLEX.1-alpha, you can bypass the guidance embedding which is the recommended way to train that model.\n#bypass_guidance_embedding = true\n```\nFor Flux, you can override the transformer weights by setting transformer_path to an original Black Forest Labs (BFL) format checkpoint. For example, the above config loads the model from Diffusers format FLUX.1-dev, but the transformer_path, if uncommented, loads the transformer from Flux Dev De-distill.\n\nFlux LoRAs are saved in Diffusers format.\n\n## LTX-Video\n```\n[model]\ntype = 'ltx-video'\ndiffusers_path = '/data2/imagegen_models/LTX-Video'\n# Point this to one of the single checkpoint files to load the transformer and VAE from it.\nsingle_file_path = '/data2/imagegen_models/LTX-Video/ltx-video-2b-v0.9.1.safetensors'\ndtype = 'bfloat16'\n# Can load the transformer in fp8.\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n# Probability to use the first video frame as conditioning (i.e. i2v training).\n#first_frame_conditioning_p = 1.0\n```\nYou can train the more recent LTX-Video versions by using single_file_path. Note that you will still need to set diffusers_path to the original model folder (it gets the text encoder from here). Only t2i and t2v training is supported.\n\nLTX-Video LoRAs are saved in ComfyUI format.\n\n## HunyuanVideo\n```\n[model]\ntype = 'hunyuan-video'\n# Can load Hunyuan Video entirely from the ckpt path set up for the official inference scripts.\n#ckpt_path = '/home/anon/HunyuanVideo/ckpts'\n# Or you can load it by pointing to all the ComfyUI files.\ntransformer_path = '/data2/imagegen_models/hunyuan_video_comfyui/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors'\nvae_path = '/data2/imagegen_models/hunyuan_video_comfyui/hunyuan_video_vae_bf16.safetensors'\nllm_path = '/data2/imagegen_models/hunyuan_video_comfyui/llava-llama-3-8b-text-encoder-tokenizer'\nclip_path = '/data2/imagegen_models/hunyuan_video_comfyui/clip-vit-large-patch14'\n# Base dtype used for all models.\ndtype = 'bfloat16'\n# Hunyuan Video supports fp8 for the transformer when training LoRA.\ntransformer_dtype = 'float8'\n# How to sample timesteps to train on. Can be logit_normal or uniform.\ntimestep_sample_method = 'logit_normal'\n```\nHunyuanVideo LoRAs are saved in a Diffusers-style format. The keys are named according to the original model, and prefixed with \"transformer.\". This format will directly work with ComfyUI.\n\n## Cosmos\n```\n[model]\ntype = 'cosmos'\n# Point these paths at the ComfyUI files.\ntransformer_path = '/data2/imagegen_models/cosmos/cosmos-1.0-diffusion-7b-text2world.pt'\nvae_path = '/data2/imagegen_models/cosmos/cosmos_cv8x8x8_1.0.safetensors'\ntext_encoder_path = '/data2/imagegen_models/cosmos/oldt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\n```\nTentative support is added for Cosmos (text2world diffusion variants). Compared to HunyuanVideo, Cosmos is not good for fine-tuning on commodity hardware.\n\n1. Cosmos supports a fixed, limited set of resolutions and frame lengths. Because of this, the 7b model is actually slower to train than HunyuanVideo (12b parameters), because you can't get away with training on lower-resolution images like you can with Hunyuan. And video training is nearly impossible unless you have enormous amounts of VRAM, because for videos you must use the full 121 frame length.\n2. Cosmos seems much worse at generalizing from image-only training to video.\n3. The Cosmos base model is much more limited in the types of content that it knows, which makes fine tuning for most concepts more difficult.\n\nI will likely not be actively supporting Cosmos going forward. All the pieces are there, and if you really want to try training it you can. But don't expect me to spend time trying to fix things if something doesn't work right.\n\nCosmos LoRAs are saved in ComfyUI format.\n\n## Lumina Image 2.0\n```\n[model]\ntype = 'lumina_2'\n# Point these paths at the ComfyUI files.\ntransformer_path = '/data2/imagegen_models/lumina-2-single-files/lumina_2_model_bf16.safetensors'\nllm_path = '/data2/imagegen_models/lumina-2-single-files/gemma_2_2b_fp16.safetensors'\nvae_path = '/data2/imagegen_models/lumina-2-single-files/flux_vae.safetensors'\ndtype = 'bfloat16'\nlumina_shift = true\n```\nSee the [Lumina 2 example dataset config](../examples/recommended_lumina_dataset_config.toml) which shows how to add a caption prefix and contains the recommended resolution settings.\n\nIn addition to LoRA, Lumina 2 supports full fine tuning. It can be fine tuned at 1024x1024 resolution on a single 24GB GPU. For FFT, delete or comment out the [adapter] block in the config. If doing FFT with 24GB VRAM, you will need to use an alternative optimizer to lower VRAM use:\n```\n[optimizer]\ntype = 'adamw8bitkahan'\nlr = 5e-6\nbetas = [0.9, 0.99]\nweight_decay = 0.01\neps = 1e-8\ngradient_release = true\n```\n\nThis uses a custom AdamW8bit optimizer with Kahan summation (required for proper bf16 training), and it enables an experimental gradient release for more VRAM saving. If you are training only at 512 resolution, you can remove the gradient release part. If you have a >24GB GPU, or multiple GPUs and use pipeline parallelism, you can perhaps just use the normal adamw_optimi optimizer type.\n\nLumina 2 LoRAs are saved in ComfyUI format.\n\n## Wan2.1\n```\n[model]\ntype = 'wan'\nckpt_path = '/data2/imagegen_models/Wan2.1-T2V-1.3B'\ndtype = 'bfloat16'\n# You can use fp8 for the transformer when training LoRA.\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\n\nBoth t2v and i2v Wan2.1 variants are supported. Set ckpt_path to the original model checkpoint directory, e.g. [Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B).\n\n(Optional) You may skip downloading the transformer and UMT5 text encoder from the original checkpoint, and instead pass in paths to the ComfyUI safetensors files instead.\n\nDownload checkpoint but skip the transformer and UMT5:\n```\nhuggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir Wan2.1-T2V-1.3B --exclude \"diffusion_pytorch_model*\" \"models_t5*\"\n```\n\nThen use this config:\n```\n[model]\ntype = 'wan'\nckpt_path = '/data2/imagegen_models/Wan2.1-T2V-1.3B'\ntransformer_path = '/data2/imagegen_models/wan_comfyui/wan2.1_t2v_1.3B_bf16.safetensors'\nllm_path = '/data2/imagegen_models/wan_comfyui/wrapper/umt5-xxl-enc-bf16.safetensors'\ndtype = 'bfloat16'\n# You can use fp8 for the transformer when training LoRA.\n#transformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nYou still need ckpt_path, it's just that it can be missing the transformer files and/or UMT5. The transformer/UMT5 can be loaded from the native ComfyUI repackaged file, or the file for Kijai's wrapper extension. Additionally, you can mix and match components, for example, using the transformer from the ComfyUI repackaged repository alongside the UMT5 safetensors from Kijai's wrapper repository for training or other combinations.\n\nFor i2v training, you **MUST** train on a dataset of only videos. The training script will crash with an error otherwise. The first frame of each video clip is used as the image conditioning, and the model is trained to predict the rest of the video. Please pay attention to the video_clip_mode setting. It defaults to 'single_beginning' if unset, which is reasonable for i2v training, but if you set it to something else during t2v training it may not be what you want for i2v. Only the 14B model has an i2v variant, and it requires training on videos, so VRAM requirements are high. Use block swapping as needed if you don't have enough VRAM.\n\nWan2.1 LoRAs are saved in ComfyUI format.\n\n## Chroma\n```\n[model]\ntype = 'chroma'\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\ntransformer_path = '/data2/imagegen_models/chroma/chroma-unlocked-v10.safetensors'\ndtype = 'bfloat16'\n# You can optionally load the transformer in fp8 when training LoRAs.\ntransformer_dtype = 'float8'\nflux_shift = true\n```\nChroma is a model that is architecturally modifed and finetuned from Flux Schnell. The modifications are significant enough that it has its own model type. Set transformer_path to the Chroma single model file, and set diffusers_path to either Flux Dev or Schnell Diffusers folder (the Diffusers model is needed for loading the VAE and text encoder).\n\nChroma LoRAs are saved in ComfyUI format.\n\n## HiDream\n```\n[model]\ntype = 'hidream'\ndiffusers_path = '/data/imagegen_models/HiDream-I1-Full'\nllama3_path = '/data2/models/Meta-Llama-3.1-8B-Instruct'\nllama3_4bit = true\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n# Can use nf4 quantization for even more VRAM saving.\n#transformer_dtype = 'nf4'\nmax_llama3_sequence_length = 128\n# Can use a resolution-dependent timestep shift, like Flux. Unsure if results are better.\n#flux_shift = true\n```\n\nOnly the Full version is tested. Dev and Fast likely will not work properly due to being distilled, and because you can't set the guidance value.\n\n**HiDream doesn't perform well at resolutions under 1024**. The model uses the same training objective and VAE as Flux, so the loss values are directly comparable between the two. When I compare with Flux, there is moderate degradation in the loss value at 768 resolution. There is severe degradation in the loss value at 512 resolution, and inference at 512 produces completely fried images.\n\nThe official inference code uses a max sequence length of 128 for all text encoders. You can change the sequence length of llama3 (which carries almost all the weight) by changing max_llama3_sequence_length. A value of 256 causes a slight increase in stabilized validation loss of the model before any training happens, so there is some quality degradation. If you have many captions longer than 128 tokens, it may be worth increasing this value, but this is untested. I would not increase it beyond 256.\n\nDue to how the Llama3 text embeddings are computed, the Llama3 text encoder must be kept loaded and its embeddings computed during training, rather than being pre-cached. Otherwise the cache would use an enormous amount of space on disk. This increases memory use, but you can have Llama3 in 4bit with essentially 0 measurable effect on validation loss.\n\nWithout block swapping, you will need 48GB VRAM, or 2x24GB with pipeline parallelism. With enough block swapping you can train on a single 24GB GPU. Using nf4 quantization also allows training with 24GB, but there may be some quality decrease.\n\nHiDream LoRAs are saved in ComfyUI format.\n\n## Stable Diffusion 3\n```\n[model]\ntype = 'sd3'\ndiffusers_path = '/data2/imagegen_models/stable-diffusion-3.5-medium'\ndtype = 'bfloat16'\n#transformer_dtype = 'float8'\n#flux_shift = true\n```\n\nStable Diffusion 3 LoRA training is supported. You need the full Diffusers folder for the model. Tested on SD3.5 Medium and Large.\n\nSD3 LoRAs are saved in Diffusers format. This format works in ComfyUI.\n\n## Cosmos-Predict2\n```\n[model]\ntype = 'cosmos_predict2'\ntransformer_path = '/data2/imagegen_models/Cosmos-Predict2-2B-Text2Image/model.pt'\nvae_path = '/data2/imagegen_models/comfyui-models/wan_2.1_vae.safetensors'\nt5_path = '/data2/imagegen_models/comfyui-models/oldt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\n#transformer_dtype = 'float8_e5m2'\n```\n\nCosmos-Predict2 supports LoRA and full fine tuning. Currently only for the t2i model variants.\n\nSet transformer_path to the original model checkpoint, vae_path to the ComfyUI Wan VAE, and t5_path to the ComfyUI [old T5 model file](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/blob/main/text_encoders/oldt5_xxl_fp16.safetensors). Please note this is the OLDER version of T5, not the one that is more commonly used with other models.\n\nThis model appears more sensitive to fp8 / quantization than most models. float8_e4m3fn WILL NOT work well. If you are using fp8 transformer, use float8_e5m2 as in the config above. Probably avoid using fp8 on the 2B model if you can. float8_e5m2 on the 14B transformer seems fine, and is required for training on a 24GB GPU.\n\nfloat8_e5m2 is also the only fp8 datatype that works for inference (as of this writing). But beware, in ComfyUI, **LoRAs don't work well when applied on a float8_e5m2 model**. The generated images are very noisy. I guess the stochastic rounding when merging the LoRA weights with this datatype just introduces too much noise. This issue doesn't affect training because the LoRA weights are separate and not merged during training. TLDR: you can use ```transformer_dtype = 'float8_e5m2'``` for training LoRAs for the 14B, but don't use fp8 on this model when applying LoRAs in ComfyUI. UPDATE: LoRAs will work fine for inference using GGUF model weights, because in that case the LoRAs aren't merged into the quantized weights.\n\nCosmos-Predict2 LoRAs are saved in ComfyUI format.\n\n## OmniGen2\n```\n[model]\ntype = 'omnigen2'\ndiffusers_path = '/data2/imagegen_models/OmniGen2'\ndtype = 'bfloat16'\n#flux_shift = true\n```\n\nOmniGen2 LoRA training is supported. Set ```diffusers_path``` to the original model checkpoint directory. Only t2i training (i.e. single image and caption) is supported.\n\nOmniGen2 LoRAs are saved in ComfyUI format.\n\n## Flux Kontext\n```\n[model]\ntype = 'flux'\n# Or just point to Flux Kontext Diffusers folder without needing transformer_path\ndiffusers_path = '/data2/imagegen_models/FLUX.1-dev'\ntransformer_path = '/data2/imagegen_models/flux-dev-single-files/flux1-kontext-dev.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n#flux_shift = true\n```\n\nFlux Kontext is supported, both for standard t2i datasets and edit datasets. The weight shapes are 100% compatible with Flux Dev, so if you already have the Dev Diffusers folder you can use transformer_path to point to the Kontext single model file to save space.\n\nSee the [Flux Kontext example dataset config](../examples/flux_kontext_dataset.toml) for how to configure the dataset.\n\n**IMPORTANT**: The control/context images should be approximately the same aspect ratio as the target images. All of the aspect ratio and size bucketing is done with respect to the target images. Then, the control image is resized and cropped to match the target image size. If the aspect ratio of the control image is very different from the target image, it will be cropping away a lot of the control image.\n\nFlux Kontext LoRAs are saved in Diffusers format, which will work in ComfyUI.\n\n## Wan2.2\nLoad from checkpoint:\n```\n[model]\ntype = 'wan'\nckpt_path = '/data/imagegen_models/Wan2.2-T2V-A14B'\ntransformer_path = '/data/imagegen_models/Wan2.2-T2V-A14B/low_noise_model'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\nmin_t = 0\nmax_t = 0.875\n```\nOr, load from ComfyUI files to save space:\n```\n[model]\ntype = 'wan'\nckpt_path = '/data/imagegen_models/Wan2.2-T2V-A14B'\ntransformer_path = '/data/imagegen_models/comfyui-models/wan2.2_t2v_low_noise_14B_fp16.safetensors'\nllm_path = '/data2/imagegen_models/comfyui-models/umt5_xxl_fp16.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n```\n\nThe 5B model is also supported, but only for t2v / t2i training, not i2v.\n\nThe LoRAs are saved in ComfyUI format.\n\n### Notes on loading models\nWhen loading from ComfyUI files, you still need the checkpoint folder with the VAE and config files inside it, but it doesn't need the transformer or T5. You can download it and skip those files like this:\n```\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir Wan2.2-T2V-A14B --exclude \"models_t5*\" \"*/diffusion_pytorch_model*\"\n```\nFor Wan2.2 A14B, if you are loading fully from the checkpoint folder, you need to use ```transformer_path``` to point to the subfolder of the model you want to train, i.e. low noise or high noise.\n\n### Timestep ranges\nWan2.2 A14B has two models: low noise and high noise. They process different parts of the timestep range during inference, switching between models once the timestep reaches a certain boundary. t=0 is no noise, t=1 is fully noise. The models are independent; you can train LoRAs for either one, or both.\n\nI couldn't find any exact details on what timesteps the Wan team used to train each model, but presumably they trained it to match how it would be used at inference time. For the T2V model, the configured inference boundary timestep is 0.875. For I2V, it is 0.9. You can (and should) use the ```min_t``` and ```max_t``` parameters to restrict the training timestep range appropriate for the model. For example, the first model config above has the timestep range set for the low noise T2V model. I don't know if the training timestep range should exactly match the inference boundary or not. For the high noise T2V model, you would use:\n```\nmin_t = 0.875\nmax_t = 1\n```\nControlling the timestep range like this will work correctly even if you are using the ```shift``` or ```flux_shift``` parameters to shift the timestep distribution.\n\nAlternatively, people have noticed that the low noise model can be used entirely on its own. So you could just train the low noise model without restricting the timestep range, just like you would do with Wan2.1.\n\n## Qwen-Image\n```\n[model]\ntype = 'qwen_image'\ndiffusers_path = '/data/imagegen_models/Qwen-Image'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nOr load from individual files:\n```\n[model]\ntype = 'qwen_image'\ntransformer_path = '/data/imagegen_models/comfyui-models/qwen_image_bf16.safetensors'\ntext_encoder_path = '/data/imagegen_models/comfyui-models/qwen_2.5_vl_7b.safetensors'\nvae_path = '/data/imagegen_models/Qwen-Image/vae/diffusion_pytorch_model.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nIn the second format, ```transformer_path``` and ```text_encoder_path``` should be the ComfyUI files, but ```vae_path``` needs to be the **Diffusers VAE** (the weight key names are completely different and the ComfyUI VAE isn't currently supported). You should use bf16 files even if you are casting the transformer to float8; fp8_scaled weights won't work at all, and fp8 weights might have slightly lower quality because the training script tries to keep some weights in higher precision. If you give both ```diffusers_path``` and the individual model paths, it will prefer to read the sub-model from the individual path.\n\nAs of this writing you will need the latest Diffusers:\n```\npip uninstall diffusers\npip install git+https://github.com/huggingface/diffusers\n```\n\nQwen-Image LoRAs are saved in ComfyUI format.\n\n### Training LoRAs on a single 24GB GPU\n- You will need block swapping. See the [example 24GB VRAM config](../examples/qwen_image_24gb_vram.toml) which has everything set correctly.\n- Use the expandable segments CUDA feature: ```PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" deepspeed --num_gpus=1 train.py --deepspeed --config /home/anon/code/diffusion-pipe-configs/tmp.toml```\n- Use a dataset resolution of 640. This is one of the resolutions the model was trained with and might work a bit better than 512.\n- If you use higher LoRA rank or higher resolution, you might need to increase blocks_to_swap.\n\n## Qwen-Image-Edit\n```\n[model]\ntype = 'qwen_image'\ndiffusers_path = '/data/imagegen_models/Qwen-Image'  # or, Qwen-Image-Edit Diffusers folder\n# Only needed if you are using Qwen-Image Diffusers model instead of Qwen-Image-Edit\ntransformer_path = '/data/imagegen_models/comfyui-models/qwen_image_edit_bf16.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\ntimestep_sample_method = 'logit_normal'\n```\nConfiguring and training Qwen-Image-Edit is the same as Flux-Kontext. See the [example dataset config](../examples/flux_kontext_dataset.toml). The same dataset considerations apply. The reference images are resized to whatever size bucket the target images end up in, so your reference images need to have approximately the same aspect ratio as the targets, or else they will be overly cropped.\n\nThe model is taking larger inputs than T2I training, so it is slower and uses more VRAM. I don't know if you can train it on 24GB VRAM. Maybe if you block swap enough.\n\nQwen-Image-Edit LoRAs are saved in ComfyUI format.\n\n\n## HunyuanImage-2.1\nUse ComfyUI compatible model files.\n```\n[model]\ntype = 'hunyuan_image'\ntransformer_path = '/data/imagegen_models/comfyui-models/hunyuanimage2.1.safetensors'\nvae_path = '/data/imagegen_models/comfyui-models/hunyuan_image_2.1_vae_fp16.safetensors'\ntext_encoder_path = '/data/imagegen_models/comfyui-models/qwen_2.5_vl_7b.safetensors'\nbyt5_path = '/data/imagegen_models/comfyui-models/byt5_small_glyphxl_fp16.safetensors'\ndtype = 'bfloat16'\ntransformer_dtype = 'float8'\n```\n\n### A note on image resolution\nDue to the high spatial compression of the VAE and the architecture of the DiT model, the compute and memory requirements at a certain image resolution are the same as half the image side length for other models. That is, 1024 resolution for Hunyuan is the same compute as 512 for Flux, Qwen, Lumina, etc. You can train at 512 resolution for a speed boost, and it does seem to learn mostly fine from this resolution even though it is relatively low for this model. But depending on the dataset, it may be better to train at 1024+, especially if you are trying to learn unique fine-grained details from your dataset.\n\nHunyuanImage-2.1 LoRAs are saved in ComfyUI format. Notably, this means some of the key names are different from the original model structure. Keep this in mind if you are trying to use the LoRA anywhere but ComfyUI.\n\n\n\n\n"
      ],
      "color": "#323",
      "bgcolor": "#535"
    }
  ],
  "links": [
    [
      190,
      91,
      0,
      177,
      2,
      "ar_buckets"
    ],
    [
      278,
      177,
      0,
      220,
      0,
      "DATASET_CONFIG"
    ],
    [
      280,
      220,
      0,
      180,
      0,
      "*"
    ],
    [
      328,
      229,
      0,
      230,
      0,
      "STRING"
    ],
    [
      338,
      75,
      0,
      177,
      0,
      "input_path"
    ],
    [
      382,
      220,
      1,
      188,
      0,
      "*"
    ],
    [
      394,
      193,
      0,
      176,
      0,
      "model_path"
    ],
    [
      395,
      193,
      0,
      279,
      0,
      "*"
    ],
    [
      398,
      230,
      1,
      283,
      0,
      "STRING"
    ],
    [
      399,
      230,
      0,
      284,
      0,
      "STRING"
    ],
    [
      428,
      177,
      0,
      291,
      0,
      "*"
    ],
    [
      429,
      174,
      0,
      292,
      0,
      "OPTIMIZER_CONFIG"
    ],
    [
      430,
      176,
      0,
      292,
      1,
      "model_config"
    ],
    [
      431,
      177,
      0,
      292,
      2,
      "DATASET_CONFIG"
    ],
    [
      432,
      175,
      0,
      292,
      3,
      "ADAPTER_CONFIG"
    ],
    [
      433,
      282,
      0,
      292,
      4,
      "ADVANCED_TRAIN_CONFIG"
    ],
    [
      434,
      292,
      0,
      220,
      1,
      "TRAIN_CONFIG"
    ],
    [
      435,
      292,
      0,
      289,
      0,
      "*"
    ],
    [
      436,
      292,
      1,
      229,
      0,
      "STRING"
    ],
    [
      437,
      292,
      2,
      220,
      2,
      "config_path"
    ]
  ],
  "groups": [
    {
      "id": 1,
      "title": "dateset",
      "bounding": [
        -5030,
        -1280,
        1450,
        1250
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 2,
      "title": "model_loader",
      "bounding": [
        -3540,
        -3270,
        1940,
        1380
      ],
      "color": "#88A",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 3,
      "title": "trainconfigæ¨¡å‹é…ç½®",
      "bounding": [
        -3520,
        -1650,
        1557.999755859375,
        1852.770751953125
      ],
      "color": "#8A8",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 4,
      "title": "go train",
      "bounding": [
        -1950,
        -1300,
        1183.061767578125,
        846.7456665039062
      ],
      "color": "#A88",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 5,
      "title": "TensorBoard",
      "bounding": [
        -1950,
        -430,
        1212.814453125,
        819.6329345703125
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    }
  ],
  "config": {},
  "extra": {
    "ue_links": [],
    "ds": {
      "scale": 0.45,
      "offset": [
        4229.847961237401,
        2337.6053436656057
      ]
    },
    "links_added_by_ue": [],
    "frontendVersion": "1.26.13",
    "VHS_latentpreview": true,
    "VHS_latentpreviewrate": 0,
    "VHS_MetadataImage": true,
    "VHS_KeepIntermediate": true
  },
  "version": 0.4
}